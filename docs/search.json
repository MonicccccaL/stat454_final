{
  "articles": [
    {
      "path": "about.html",
      "title": "Introduction and Motivation",
      "author": [],
      "contents": "\nIntroduction\n\nCritical thinking is an active and ongoing process. It requires that we all think like Bayesians, updating our knowledge as new information comes in.\n\n— Daniel J. Levitin, A Field Guide to Lies: Critical Thinking in the Information Age\n\n\nWhen analyzing data, particularly spatial data, it’s crucial to account for complex dependency relationships and spatial autocorrelation. Spatial data often exhibit patterns where nearby locations are correlated more to each other than locations farther apart. A popular spatial model used is the Besag-York-Mollié (BYM) model (Besag, York, and Mollié 1991), which takes spatial correlation into account. The similar observations in neighboring areas violate the assumption of independence commonly made in traditional statistical models. Here is where hierarchical models come in to solve the problem!\nHierarchical models offer a compelling solution to this challenge by striking a balance between recognizing individuality and shared information among groups within the data. In the realm of spatial data analysis, hierarchical models shine, allowing us to seamlessly incorporate spatial structure and dependencies. These models not only capture the variability in response variables based on known covariates but also accommodate residual variation through random effects. This flexibility empowers us to assess explanatory variables, address spatial autocorrelation, and quantify uncertainty with precision.\nIn the realm of statistics, our journey is guided by a shared quest to glean insights from data and better understand the world around us. While frequentist analysis, prevalent in introductory statistics courses, focuses on the properties of observed data, Bayesian analysis takes a broader perspective. It integrates prior beliefs with observed data, leveraging the power of probability distributions to quantify uncertainty. The cornerstone of Bayesian inference is the Bayes’ Theorem, where: \\[P(A|B)=\\frac{P(B|A)P(A)}{P(B)}.\\]\nIn the context of spatial modeling, Bayesian hierarchical models are particularly advantageous. They allow for the incorporation of prior information, spatial structure, and dependencies, providing a comprehensive framework for spatial data analysis. One notable approach in this domain is the integrated nested Laplace approximation (INLA), which combines analytically approximations and numerical algorithms to approximate posterior distributions with closed-form expressions. INLA is especially useful for fitting spatial models efficiently and accurately, making it a valuable tool for spatial data analysis in various research fields!\nMotivation of the project\nForecasting future outcomes is a crucial objective across numerous fields, encompassing economics, healthcare, and various social sciences. The capacity of Bayesian hierarchical models to amalgamate prior knowledge with observed data while quantifying uncertainty and enabling flexible model specification is pivotal for their utility across diverse disciplines. Xiang and Cynthia both come from an economic background and have some knowledge of spatial modeling from the course Correlated Data (F23). We teamed up and decided to investigate a novel, expedited Bayesian approach, integrated nested Laplace approximation (INLA), which integrates spatial data and modeling techniques and employs this Bayesian spatial model to address economic inquiries.\n\n\n\n",
      "last_modified": "2024-04-23T15:06:35-05:00"
    },
    {
      "path": "bayes.html",
      "title": "Bayesian Inference and INLA",
      "author": [],
      "contents": "\nBayesian Inference\nBayesian inference seeks to derive predictive or posterior distributions\nfor models, representing the probability of parameters given observed\ndata, or for predicting new data. The posterior distribution is computed\nby multiplying the data likelihood with the prior distributions and\nnormalizing them to integrate into one.\nIn a Bayesian framework, we first specify a likelihood function\n\\(\\pi(y|\\theta)\\), describing the probability of observing data\n\\(y = (y_1, \\dots, y_n)\\) given parameters \\(\\theta\\). We assign a prior\ndistribution \\(\\pi(\\theta|\\eta)\\) to \\(\\theta\\), where \\(\\eta\\) represents\nhyperparameters governing the parameter distribution. These priors\nencode our knowledge about \\(\\theta\\) before observing \\(y\\). If\nhyperparameters \\(\\eta\\) are unknown, a fully Bayesian approach involves\nspecifying a hyperprior distribution for \\(\\eta\\), or employing empirical\nBayes with estimated \\(\\eta\\). Assuming \\(\\eta\\) is known, inference about\n\\(\\theta\\) relies on Bayes’ Theorem: \\[\n\\pi ( \\theta | y ) = \\frac{\\pi( y | \\theta )  \\pi(\\theta)}{ \\int \\pi ( y | \\theta) \\pi ( \\theta ) d \\theta}\n\\] The denominator represents the marginal likelihood of \\(y\\), acting as\na scaling constant, leading to the proportional posterior distribution:\n\\[\n\\pi ( \\theta | y ) \\propto \\pi( y | \\theta )  \\pi(\\theta)\n\\]\nBayesian methods integrate prior beliefs into the model, updating them\nwith observed data and accommodating complex models. However, the\nintegral in the denominator is often intractable, so methods like MCMC\nare employed to sample from conditionals and estimate the marginal\ndistribution for each parameter of interest.\nWhat is Markov Chain Monte Carlo (MCMC)?\nWhile there are no closed-form expressions for posterior distribution,\nMarkov chain Monte Carlo (MCMC) methods have been traditionally used to\nsolve this problem. MCMC is a simulation method, addressing cases where\nclosed-form expressions for posterior distributions are unavailable.\nThis is also the method we will generally learn in class. It simulates a\nMarkov chain that converges to the desired posterior distribution by\ngenerating a sequence of parameter values \\(\\theta^{(j)}\\) for\n\\(j = 1, ..., n\\). MCMC avoids explicit integration of the challenging or\nimpossible marginal likelihood function by proposing new parameter\nvalues based on current ones, guided by an acceptance probability\nensuring convergence to the stationary distribution.\nOnce convergence is achieved, sampled parameter values estimate key\ncharacteristics of the posterior distribution. Diagnostics like trace\nplots monitor mixing and convergence, ensuring reliable inference. MCMC\nprovides a robust framework for Bayesian inference, enabling efficient\nanalysis of complex statistical problems. The MCMC method has made a\ngreat impact on statistical practice by making Bayesian inference\npossible for complex models and high-dimensional data. However, since it\nis a sampling simulation method that is extremely computationally\ndemanding, it may be slow to converge and can take a long time to\nexecute.\nAn Alternative – INLA Method\nIntegrated nested Laplace approximation (INLA) offers a\ncomputationally efficient alternative to MCMC for approximate Bayesian\ninference in latent Gaussian models (Håvard Rue, Martino, and Chopin\n2009). While MCMC relies on the convergence of a Markov chain to\napproximate the desired posterior distribution, INLA takes a different\napproach. It employs a Laplacian approximation to estimate the\nindividual posterior marginals of model parameters. This method combines\nanalytical approximations with numerical algorithms for sparse matrices,\nenabling the approximation of posterior distributions with closed-form\nexpressions. This facilitates faster inference and avoids issues related\nto sample convergence in MCMC, allowing for the analysis of large\ndatasets and exploration of alternative models.\nThe INLA framework entails a systematic approach to understanding and\nmodeling relationships within observed variables. It begins with a\nvector of observed variables, \\(y = (y_1, \\ldots, y_n)\\), typically\nfollowing the exponential family distribution. Each observation, \\(y_i\\),\nis associated with a mean value, \\(\\mu_i\\), linked to a linear predictor,\n\\(\\eta_i\\), through a suitable link function. This linkage facilitates the\nconnection between observed means and the underlying linear structure,\ncrucial for understanding data relationships. The linear predictor,\n\\(\\eta_i\\), encompasses various terms, including covariates (fixed\neffects) and different random effects, forming the basis of the linear\nstructure. A vector \\(x\\) represents all latent effects, including the\nlinear predictor and covariate coefficients, capturing the underlying\ndata structure.\nAdditionally, the distribution of observed variables, \\(y\\), may depend on\na vector of hyperparameters, \\(\\theta_1\\), governing data variability.\nThis comprehensive approach enables effective analysis and\ninterpretation of observed data, revealing meaningful insights and\nrelationships inherent in the data’s structure. The distribution of the\nlatent effects \\(x\\) is assumed to be Gaussian Markov random field (GMRF).\nThis GMRF will have a zero mean and precision matrix \\(Q(\\theta_2)\\), with\n\\(\\theta_2\\) a vector of hyperparameters. The vector of all\nhyperparameters in the model will be denoted by\n\\(\\theta = (\\theta_1, \\theta_2)\\).\nMoreover, given the vector of latent effects, \\(x\\), and the\nhyperparameters, \\(\\theta\\), it is assumed that the observations are\nindependent of each other. This implies that the likelihood function\ncan be expressed as:\n\\[\n\\pi(y|x,\\theta)=\\prod_{i \\in \\mathcal {I}} \\pi(y_i|\\eta_i, \\theta)\n\\]\nwhere,\n\\(\\eta_i\\): the latent linear predictor (which is part of the vector\n\\(x\\) of latent effects). It serves as a representation of the\nunderlying linear structure or trend that influences the observed\ndata \\(y_i\\).\n\\(\\mathcal{I}\\): contains the indices for all observed values of \\(y\\).\nIn other words, the indices of the observations for which we have\nactual data.\nThe aim of the INLA methodology is to approximate the posterior\nmarginals of model parameters and hyperparameters, which estimates the\ndistribution of each parameter and hyperparameter separately within the\noverall posterior distribution, allowing us to understand the\nuncertainty associated with each parameter and hyperparameter\nindependently. This is achieved by exploiting the computational\nproperties of GMRF and the Laplace approximation for multidimensional\nintegration.\nThe joint posterior distribution of the effects and hyperparameters can\nbe expressed as:\n\\[\n\\pi(x,\\theta|y)\\propto \\pi(θ)\\pi(x|\\theta) \\prod_{i \\in \\mathcal {I}} \\pi(y_i|x_i, \\theta)\\\\\n\\propto \\pi(\\theta)|Q(\\theta)|^{\\frac{1}{2}} exp \\{ −\\frac{1}{2}x^\\top Q(\\theta)x + \\sum_{i \\in \\mathcal{I}}log(\\pi(y_i|x_i,\\theta)) \\}.\n\\]\nwhere,\n\\(Q(\\theta)\\): the precision matrix of the latent effects. The\nprecision matrix is the inverse of the covariance matrix, which\nintuitively, it quantifies the relationships and dependencies\nbetween parameters and hyperparameters in the model. This allows us\nto model complex interactions and correlations in the data.\n\\(x_i = \\eta_i\\) when \\(i \\in \\mathcal{I}\\), since mentioned above\n\\(\\eta_i\\) is part of vector \\(x\\). This implies that for observations\nin the set of observed values, the latent effects \\(x_i\\)​ are directly\nequal to the corresponding latent linear predictors \\(\\eta_i\\).\nNow we can calculate the marginal distributions (MAIN GOAL!!) for\nthe latent effects and hyperparameters. It can be done considering that\n\\[\n\\pi(x_i|y)= \\int \\pi(x_i|\\theta,y)\\pi(\\theta|y)d\\theta\\\\\n\\pi(\\theta_j | y ) = \\int \\pi ( \\theta | y ) d\\theta_ {−j} \\\\\n\\text{where} \\ \\theta_{-j}=(\\theta_1, \\dots, \\theta_{j−1}, \\theta_{j+1}, \\dots, \\theta_m).\n\\]\n​This provides us a way to obtain the marginal distribution of the latent\neffect \\(x_i\\) (model parameters) and a specific hyperparameter \\(\\theta_j\\)​\nfrom the joint posterior distribution of all hyperparameters given the\nobserved data \\(y\\).\nThis nested formulation, which reflects the layered approach of\napproximating the posterior distributions of latent effects and\nhyperparameters, is used to approximate \\(\\pi(x_i|y)\\) by combining\nanalytical approximations to the full conditionals \\(\\pi(x_i|\\theta,y)\\)\nand \\(\\pi(\\theta|y)\\) and numerical integration routines to integrate out\n\\(\\theta\\). Similarly, \\(\\pi(\\theta_j|y)\\) is approximated by approximating\n\\(\\pi(\\theta|y)\\) and integrating out \\(\\theta_{-j}\\). To construct nested\napproximations by INLA, it can be written as\n\\[\n\\tilde \\pi(x_i|y)= \\int \\tilde \\pi(x_i|\\theta,y) \\tilde \\pi(\\theta|y)d\\theta\\\\\n\\tilde\\pi(\\theta_j | y ) = \\int \\tilde \\pi ( \\theta | y ) d\\theta_ {−j}\n\\]\nwhere \\(\\tilde \\pi( \\cdot| \\cdot)\\) is an approximated posterior density.\nThe approximation to the marginal density \\(\\pi(x_i|y)\\) is obtained in a\nnested fashion by first approximating \\(\\pi(\\theta|y)\\) and\n\\(\\pi(x_i|\\theta,y)\\), and then numerically integrating out \\(\\theta\\) as\n\\[\n\\pi(x_i|y)=\\sum_k \\tilde \\pi(x_i|\\theta_k,y)\\times\\tilde \\pi(\\theta_k|y)\\times \\Delta k,\n\\]\nwhere\n\\(\\Delta_k\\): represents the integration weights associated with each\nvalue of \\(\\theta_k\\)​, which also denotes the area weight\ncorresponding to \\(\\theta_k\\).\n\\(\\sum _k\\): the summation of all possible values of \\(\\theta\\) with\neach term weighted by the respective integration weight\n\\(\\Delta_{k}\\).\nThe posterior marginals for the \\(x_i\\) conditioned on selected values of\n\\(\\theta_k\\)​, denoted by \\(\\pi(x_i|\\theta_k,y)\\), and the approximation\n\\(\\tilde \\pi (\\theta_k|y)\\) can be obtained using various approximation\ntechniques such as Gaussian, Laplace, or simplified Laplace\napproximations. The Gaussian approximation, derived from\n\\(\\tilde\\pi_G(x|\\theta,y)\\), is the simplest and fastest solution.\nHowever, it may introduce errors in location and fail to capture\nskewness behavior in certain situations. The Laplace approximation,\nwhile preferable to the Gaussian one, is relatively more computationally\nexpensive. Alternatively, the simplified Laplace approximation provides\na middle-ground between computational cost and accuracy, which is also\nthe default option in the R-INLA package.\nThe R-INLA Package\nThe integrated nested Laplace approximation (INLA) approach is\nimplemented in the R package R-INLA (Havard Rue, Lindgren, and\nTeixeira Krainski 2023). The INLA website\n(http://www.r-inla.org)) includes\nintroductions, instructions for download, examples, and other resources\n(i.e. discussion group, news, etc.) about INLA and the R-INLA package\n(Wang, Ryan, and Faraway 2018; Krainski et al. 2019; Moraga 2019;\nGómez-Rubio 2020).\nTo fit a model using INLA in R, we typically define the linear predictor\nas a formula object. Then, we call the inla() function (MAIN\nfunction), passing the formula, the family distribution, the data, and\nany other necessary options. This function works in a similar way as the\nglm() functions that we mostly familiar with.\n\n\n\n",
      "last_modified": "2024-04-18T15:12:12-05:00"
    },
    {
      "path": "index.html",
      "title": "Bayesian Statistics: Bayesian Spatial Model",
      "description": "Welcome to the website. This website introduces Bayesian Spatial Model in bayesian statistics. This is a class project from course STAT 454: Bayesian Statistics at Macalester College. The contents in this blog are collaborative efforts from Cynthia Zhang and Xiang Li.\n",
      "author": [],
      "contents": "\nTable of contents\nTo start exploring, please simply navigate to tabs on the upper right corner. We will start by introducing spatial model, Bayesian model, and motivation behind this project, then diving into Bayesian Spatial Modeling, and ending up with a applied data simulation study. Have fun!\nIf you have any questions, please reach out to czhang@macalester.edu and xli2@macalester.edu. Thank you!\n\n\n\n",
      "last_modified": "2024-04-23T15:05:54-05:00"
    },
    {
      "path": "sim.html",
      "title": "House pricing: simulation activity in R",
      "author": [],
      "contents": "\nSpatial Modeling of Housing Prices\nGoal: estimate housing prices in Hennepin and Ramsey county, Minneapolis, Minnesota using the BYM spatial model with R-INLA packages.\nHousing Price Data in Hennepin and Ramsey county, Minneapolis, Minnesota\nThe Hennepin and Ramsey housing prices are gathered aggregate summaries of the people who live in Hennepin and Ramsey county, summarized for each census tract based on data from 2015-2019 American Community Service run by the U.S. Census Bureau. A census tract is a statistical subdivision of a county that aims to have roughly 4,000 inhabitants and they are intended to be fairly homogeneous with respect to demographic and economic conditions. #need revision\n\n\nload('SpatialData.RData')\nggplot(mn_data, aes(x=HouseValueE))+\n  geom_density()\n\n\n\nThis dataset contains housing data of 464 Hennepin and Ramsey census tracts including median prices of housing in USD (HouseValueE), median household income in the past 12 months (in 2019 inflation) (IncomeE), and average household size of occupied houses (HouseholdSizeE). We create the variable called vble with the logarithm of the median prices to put it in the same scale, and map the log price using mapview (Figure 1). The map suggests that the housing prices are greater in the southwest, and prices are related to those in neighboring areas. Noted that the empty area is the University of Minnesota area, since there is no houses listed.\nFigure 1 Logarithm of housing prices in Hennipin & Ramsey county, MN per census tract from the spData package\n\n\nmn_data$vble <- log(mn_data$HouseValueE)\nmapview(mn_data, zcol = \"vble\")\n\n\n\nWe will model the logarithm of the median prices using as covariates the average Median household income in the past 12 months (IncomeE) and the average household size (HouseholdSizeE). Figure 2 shows the relationships between pairs of variables visualized using the ggpairs() function of the GGally package (Schloerke et al. 2021). We observe a negative relationship between the logarithm of housing price and crime, and a positive relationship between the logarithm of housing price and the average number of rooms.\nFigure 2 Relationship between the outcome variable logarithm of housing price (vble), and the average Median household income in the past 12 months (IncomeE) and the average household size (HouseholdSizeE).\n\n\nggpairs(data = mn_data, columns = c(\"vble\", \"HouseholdSizeE\", \"IncomeE\"))\n\n\n\nGeneral Model\nLet \\(Y_i\\) be the logarithm of housing price of area \\(i\\), \\(i = 1 , \\dots, n\\). We fit a BYM model that considers \\(Y_i\\) as the response variable, and household income and household size as covariates:\n\\[\nY \\sim N(\\mu_i,\\sigma^2), i = 1, \\dots, n \\\\\n\\mu_i=\\beta_0+\\beta_1 \\times Income+\\beta_2 \\times HouseholdSize + u_i+v_i\n\\]\nwhere, (Question? does this mean that each household has a seperate intercept) - \\(\\beta_0\\): the global intercept, which every household will have the same intercept. - \\(\\beta_1\\), \\(\\beta_2\\): represent respectively, the coefficients of the covariates household income and household size. - \\(\\mu_i\\): a spatially structured effect modeled with a CAR structure,\\(\\mu_i| u_{-i} \\sim N(\\mu_{\\delta_i}, \\frac{\\sigma^2_u}{n_{\\delta_i}})\\). - \\(v_i\\): an unstructured effect modeled as \\(v_i \\sim N(\\theta, \\sigma^2_v)\\).\nNeighborhood Matrix\nWe decided to use Queen neighborhood structure, which …\nFigure 3 Neighborhood Network Structure\nIn the model, the spatial random effect \\(u_i\\) needs to be specified using a neighborhood structure. As we mentioned in the spatial model section, there are 3 commmon neighborhood structure: Queen, Rook, and Bishop. Here, we assume two areas are neighbors if they share a common edge or a common vertex. We create a neighborhood structure using functions of the spdep package (Bivand 2022).\nFirst, we use the poly2nb() function to generate a spatial weights matrix (nb) based on the data (mn_data) and using queen contiguity to define neighborhood relationships between polygons. Each element of the list nb represents one area and contains the indices of its neighbors. For example, nb[[1]] contains indices of neighboring polygons in area 1.\n\n\n# Neighborhood\nnb <- poly2nb(mn_data, queen = TRUE)\nhead(nb, 3)\n\n[[1]]\n[1]  50  64 122 177 241 423 457\n\n[[2]]\n[1] 145 180 276 339 365\n\n[[3]]\n[1] 128 266 337 385 463\n\nFigure 3 shows a map with the neighbors obtained. This plot is obtained by first plotting the map of Ramsey and Hennipin, and then overlapping the queen neighborhood structure with the plot.nb() function passing the neighbor list and the coordinates of the map.\n\n\nplot(st_geometry(mn_data), border = \"black\")\nplot.nb(nb, st_geometry(mn_data), add = TRUE)\n\n\n\nTo construct the neighborhood matrix that can be used in the INLA function, we use the nb2INLA() function to convert the nb list into a file called map.adj. This step will make sure that we have the representation of the neighborhood matrix as required by R-INLA. The map.adj file is saved in the working directory. Then, we read the map.adj file using the inla.read.graph() function of R-INLA, and store it in the object g which we later use to specify neighborhood for the spatial model when using R-INLA.\n\n\nnb2INLA(\"map.adj\", nb)\ng <- inla.read.graph(filename = \"map.adj\")\n\n\ninla() Call\nWe specify the model formula by including the outcome and the fixed and random effects. In the formula, random effects are specified with the f() function. The index vectors of the random effects re_u and re_v are respectively created for spatial random effect, \\(u_i\\), and unstructured effect, \\(v_i\\). These vectors are equal to the number of areas from mn_data.\nFor \\(u_i\\) , we use model = \"besag\", a popular model for spatial autocorrelation, with neighborhood matrix given by \\(g\\) form the previous code. The option scale.model = TRUE is used to scale the precision parameter of models, making the spatial effects of models with different CAR priors comparable. For \\(v_i\\) , we choose model = \"iid\", which assuming there is no spatial autocorrelation structure.\n\n\nmn_data$re_u <- 1:nrow(mn_data)\nmn_data$re_v <- 1:nrow(mn_data) \n\n# set up spatial model\nformula <- vble ~ \n  f(re_u, model = \"besag\", graph = g, scale.model = TRUE) +\n  f(re_v, model = \"iid\")\n\n\nIn R-INLA, the BYM model can also be specified with model = \"bym\" and this comprises both the spatial and unstructured components. Alternatively, we can use the BYM2 model (Simpson et al. 2017) which is a reparametrization of the BYM model that uses a scaled spatial component \\(u_∗\\) and an unstructured component \\(v_∗\\), which allows for separate inference on the spatial and unstructured components. The formula of the model using the BYM2 component is specified as follows:\n\n\n# specify BYM\nformula <- vble ~ f(re_u, model = \"bym2\", graph = g, hyper = prior)\n\n\nThen, we fit the model by calling the inla() function specifying the formula to the spatial formula we previously specified, the family to Gaussian (normal), and the data to mn_data. Here, since we don’t have information on our prior, we will use the default priors in R-INLA. We also set control.predictor = list(compute = TRUE) and control.compute = list(return.marginals.predictor = TRUE) to compute and return the posterior means of the predictors.\n\n\nres <- inla(formula, family = \"gaussian\", data = mn_data,\n            control.predictor = list(compute = TRUE),\n            control.compute = list(return.marginals.predictor = TRUE))\n\n\n *** inla.core.safe:  The inla program failed, but will rerun in case better initial values may help. try=1/1 \nError in inla.core.safe(formula = formula, family = family, contrasts = contrasts, : object 'prior' not found\nThe inla program failed and the maximum number of tries has been reached.\n\nResults\n\n\nres$summary.fixed\n\nError in eval(expr, envir, enclos): object 'res' not found\n\nsummary(res$summary.fitted.values)\n\nError in h(simpleError(msg, call)): error in evaluating the argument 'object' in selecting a method for function 'summary': object 'res' not found\n\n# Posterior mean and 95% CI\nmn_data$PM <- res$summary.fitted.values[, \"mean\"]\n\nError in eval(expr, envir, enclos): object 'res' not found\n\nmn_data$LL <- res$summary.fitted.values[, \"0.025quant\"]\n\nError in eval(expr, envir, enclos): object 'res' not found\n\nmn_data$UL <- res$summary.fitted.values[, \"0.975quant\"]\n\nError in eval(expr, envir, enclos): object 'res' not found\n\n\n\nmapsf <- st_as_sf(mn_data)\n\ngRR <- ggplot(mn_data) + geom_sf(aes(fill = PM)) +\n  scale_fill_gradient2( midpoint = 12.5,low = \"blue\", high = \"red\") +\n  theme_bw()\ngLL <- ggplot(mn_data) + geom_sf(aes(fill = LL)) +\n  scale_fill_gradient2(midpoint = 12.5, low = \"blue\", high = \"red\") +\n  theme_bw()\ngUL <- ggplot(mn_data) + geom_sf(aes(fill = LL)) +\n  scale_fill_gradient2(midpoint = 12.5, low = \"blue\", high = \"red\") +\n  theme_bw()\n\nlibrary(cowplot)\nplot_grid(gRR, gLL, gUL, ncol = 1)\n\nError in `geom_sf()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error:\n! object 'PM' not found\n\n\n\n# common legend\nat <- seq(min(c(mn_data$PM, mn_data$LL, mn_data$UL)),\n          max(c(mn_data$PM, mn_data$LL, mn_data$UL)),\n          length.out = 8)\n\nError in seq.default(min(c(mn_data$PM, mn_data$LL, mn_data$UL)), max(c(mn_data$PM, : 'from' must be a finite number\n\n# popup table\npopuptable <- leafpop::popupTable(dplyr::mutate_if(mn_data,\n                                  is.numeric, round, digits = 2),\nzcol = c(\"NAME\", \"vble\", \"HouseholdSizeE\", \"IncomeE\", \"PM\", \"LL\", \"UL\"),\nrow.numbers = FALSE, feature.id = FALSE)\n\nError in `[.data.frame`(x, , zcol, drop = FALSE): undefined columns selected\n\nm1 <- mapview(mn_data, zcol = \"PM\", map.types = \"CartoDB.Positron\",\n              at = at, popup = popuptable)\n\nError in eval(expr, envir, enclos): object 'at' not found\n\nm2 <- mapview(mn_data, zcol = \"LL\", map.types = \"CartoDB.Positron\",\n              at = at, popup = popuptable)\n\nError in eval(expr, envir, enclos): object 'at' not found\n\nm3 <- mapview(mn_data, zcol = \"UL\", map.types = \"CartoDB.Positron\",\n              at = at, popup = popuptable)\n\nError in eval(expr, envir, enclos): object 'at' not found\n\nm <- leafsync::sync(m1, m2, m3, ncol = 3)\n\nError in eval(expr, envir, enclos): object 'm1' not found\n\nm\n\nError in eval(expr, envir, enclos): object 'm' not found\n\n\n\n# Transformation of the marginal of\n# the first area with inla.tmarginal()\n# inla.tmarginal(function(x) exp(x),\n#                res$marginals.fitted.values[[1]])\n\n# Transformation marginals with inla.tmarginal()\nmarginals <- lapply(res$marginals.fitted.values,\nFUN = function(marg){inla.tmarginal(function(x) exp(x), marg)})\n\nError in eval(expr, envir, enclos): object 'res' not found\n\n# Obtain summaries of the marginals with inla.zmarginal()\nmarginals_summaries <- lapply(marginals,\nFUN = function(marg){inla.zmarginal(marg)})\n\nError in eval(expr, envir, enclos): object 'marginals' not found\n\n# Posterior mean and 95% CI\nmn_data$PMoriginal <- sapply(marginals_summaries, '[[', \"mean\") \n\nError in eval(expr, envir, enclos): object 'marginals_summaries' not found\n\nmn_data$LLoriginal <- sapply(marginals_summaries, '[[', \"quant0.025\")\n\nError in eval(expr, envir, enclos): object 'marginals_summaries' not found\n\nmn_data$ULoriginal <- sapply(marginals_summaries, '[[', \"quant0.975\")\n\nError in eval(expr, envir, enclos): object 'marginals_summaries' not found\n\n# common legend\nat <- seq(min(c(mn_data$PMoriginal, mn_data$LLoriginal, mn_data$ULoriginal)),\n          max(c(mn_data$PMoriginal, mn_data$LLoriginal, mn_data$ULoriginal)),\n          length.out = 8)\n\nError in seq.default(min(c(mn_data$PMoriginal, mn_data$LLoriginal, mn_data$ULoriginal)), : 'from' must be a finite number\n\n# popup table\npopuptable <- leafpop::popupTable(dplyr::mutate_if(mn_data,\n                                  is.numeric, round, digits = 2),\nzcol = c(\"NAME\", \"vble\", \"HouseholdSizeE\", \"IncomeE\", \"PM\", \"LL\", \"UL\"),\nrow.numbers = FALSE, feature.id = FALSE)\n\nError in `[.data.frame`(x, , zcol, drop = FALSE): undefined columns selected\n\nm1 <- mapview(mn_data, zcol = \"PMoriginal\",\n              map.types = \"CartoDB.Positron\",\n              at = at, popup = popuptable)\n\nError in eval(expr, envir, enclos): object 'at' not found\n\nm2 <- mapview(mn_data, zcol = \"LLoriginal\",\n              map.types = \"CartoDB.Positron\",\n              at = at, popup = popuptable)\n\nError in eval(expr, envir, enclos): object 'at' not found\n\nm3 <- mapview(mn_data, zcol = \"ULoriginal\",\n              map.types = \"CartoDB.Positron\",\n              at = at, popup = popuptable)\n\nError in eval(expr, envir, enclos): object 'at' not found\n\nleafsync::sync(m1, m2, m3, ncol = 3)\n\nError in eval(expr, envir, enclos): object 'm1' not found\n\nConclusion\n\n\n\n",
      "last_modified": "2024-04-22T01:22:34-05:00"
    },
    {
      "path": "spatial.html",
      "title": "Spatial Models",
      "author": [],
      "contents": "\nNeighborhood Structure\nBefore we build a model to handle the spatial correlation between the regions, we need to first define the neighborhood structure, that is, how do we define whether two regions are spatially correlated. There are several ways to define a neighborhood and the most common one is the Queen neighborhood structure, meaning if the two polygons touch each other, even a point, they are neighbors. In the following discussion, we will apply this structure to the model. With the neighborhood structure, we can codify it with a spatial weighting matrix \\(W\\). \\(W\\) is a \\(n \\times n\\) matrix, where \\(n\\) represents the number of regions. Here we are using a binary matrix, meaning the values \\(w_{ij}\\) in the matrix are either 0 or 1. 0 means the region \\(i\\) and region \\(j\\) are not neighbors (no influence of \\(i\\) on \\(j\\)) and 1 means they are. For the Queen neighborhood structure, the matrix is symmetric by the diagonal.\nBYM Model\nThe spatial model we are using here is the Besag-York-Mollie (BYM) model (Besag, York, and Mollie 1991). Here we assume the outcome variable (\\(Y_i\\)) can be modeled using a normal distribution. \\(i \\in {1, 2, …, n}\\) represents the regions we are investigating. The BYM model can be specified as\n\\[\\begin{array}{lrl}\n\\text{Data: } & Y|\\beta_0, \\beta_k, \\sigma \\sim N(\\mu_i, \\sigma^2) \\;\\;\n\\text{ where } \\mu_i = \\beta_0 + x_{ik} \\beta_k + ø_i + \\epsilon_i \\\\\n&& \\\\\n\\text{Priors: }\n& \\beta_0 \\sim N(m_0, s_0^2) \\\\\n& \\beta_k \\sim N(m_k, s_k^2) \\\\\n& \\sigma \\sim exp(l)\\\\\n& Ø \\sim \\\\\n& \\epsilon \\sim N(0, \\sigma_\\epsilon^2)\n\\end{array}\\]\nThe model can be split into two components, fixed effect \\(\\beta_0 + x_{ik}\\beta_k\\) and random effect, \\(ø_i + \\epsilon_i\\). For the fixed effect, \\(\\beta_0\\) is the intercept, \\(\\beta_k\\) is a set of coefficients of k predictors we are interested in. The random effect includes spatial random effect \\(ø_i\\) and non-spatial random effect \\(\\epsilon_i\\). Spatial random effect \\(ø_i\\) accounts for the spatial dependence between outcomes indicating the value of a region is correlated with the value of its neighbors. And the non-spatial random effect \\(\\epsilon_i\\) models the uncorrelated noise. In the BYM model, the spatial random effect \\(ø_i\\) is modeled by the intrinsic conditional autoregressive (ICAR) model. \\(N_i\\) is the set of neighbors of region \\(i\\), \\(n\\) is the number of neighbors in this set \\(N_i\\), and \\(j\\) is the neighbor region with \\(j \\in N_i\\). We will discuss the CAR and ICAR model in detail in the following subsection.\n(Intrinsic) Conditional Autoregressive Model\nThe Conditional Autoregressive (CAR) model is one of the most widespread approaches for spatial modeling. It was first introduced by Besag (1974) and remains one of the most primary methods for areal data modeling. Generally, the CAR model relies on the binary neighborhood structure \\(W\\), which we discussed earlier. The entries \\(w_{ij}\\) is 1 if region \\(i\\) and region \\(j\\) are neighbors and 0 otherwise. The region is not considered to be the neighbor of itself, so \\(w_{ii}\\), the entries on the diagonal, is 0.\nIn the BYM model, the spatial interaction between pairs of region \\(i\\) and region \\(j\\) is modeled conditionally as a set of spatial random effects \\(ø = (ø_1, ø_2, …, ø_N)\\), \\(N\\) represents the number of regions we are looking at. The distribution of ø_i is determined by the sum of weighted values of its neighbors \\(w_{ij}ø_j\\). The model is specified as\n\\[ø_i | ø_j, j ≠ i \\sim N(\\sum w_{ij}ø_j, \\sigma_i^2)\\]\nIn the model, \\(j\\) is the neighbor region in the neighborhood set of region \\(i\\), \\(N_i\\). \\(n\\) is the count of neighbors in the set \\(N_i\\), and \\(\\sigma_i\\) is the unknown variance of random effect \\(ø_i\\).\nBesag (1974) proved that the joint distribution of \\(ø\\) is a multivariate normal centered at 0. The variance of \\(ø\\) is specified as a precision matrix \\(Q\\) which is the inverse of a covariance matrix. With that,\n\\[ø \\sim N(0, Q^{-1})\\]\nFor the multivariate normal random variable \\(ø\\), the precision matrix \\(Q\\) is constructed from two matrices: diagonal matrix \\(D\\) and adjacency matrix, which is the neighborhood matrix we defined above \\(W\\). For CAR model, \\(Q\\) is defined as\n\\[Q = D(I - \\alpha W)\\]\nwhere \\(I\\) is the identity matrix, and \\(\\alpha\\) is a parameter in CAR model controlling the amount of spatial dependence and \\(0 < \\alpha < 1\\). \\(\\alpha = 0\\) means spatial independence, \\(\\alpha = 1\\) otherwise. For Intrinsic Conditional Autoregressive (ICAR) model, we set \\(\\alpha\\) is 1 and the above equation can be simplified as\n\\[Q = D - W\\]\nIn the ICAR model, \\(ø_i\\) is normally distributed with the mean equal to the average of neighbor values. That is saying the spatial effect of region \\(i\\) is determined by the mean of its neighbors’ spatial effect. The conditional specification of ICAR model is\n\\[ø_i | ø_j, i ≠ j \\sim N(\\sum ø_j/n, \\sigma_i^2/n)\\]\n\\(\\sigma_i^2 /n\\) implies that the variance of spatial effect decreases as the number of neighbors increases. In other words, if the region has more neighbors, we are more confident in estimating its spatial effect.\nBYM2 Model\nIn the previous section, we discussed the BYM model. However, because of the lack of informative hyperpriors specific for \\(ø\\) and \\(\\epsilon\\), the BYM model could be challenging to fit. Riebler et al. (2016) further explained the issue of the BYM model that it has unnecessary long computation time when fitting the model. So, building on to the BYM model, the BYM2 model, described by Riebler et al. (2016) and Simpson et al. (2017), aims to solve these problems by adding new parameters \\(\\rho\\), \\(s\\), \\(\\sigma_c\\). It is also the final model we are using in this project.\nSame as the BYM model, the BYM2 model also includes the spatial random effect and non-spatial random effect (noise), but placing those parameters on the error terms. \\(\\sigma_c\\) is a single scale parameter placed on the combined random effect terms, meaning the standard deviation of the combined error terms. \\(\\rho \\in [0, 1]\\) is a mixing parameter for the proportion of variance coming from the spatial random effect. \\(s\\) is a scaling factor scaling the proportion \\(\\rho\\) to make the standard deviation of the combined error terms \\(\\sigma_c\\) legitimately. With all those, the BYM2 model is specified as\n\\[\\begin{array}{lrl}\n\\end{array}\\]\nWith all those theoretical background of the BYM2 model, in the following sections, we will apply it to real-world data for posterior simulation.\n\n\n\n",
      "last_modified": "2024-04-23T15:06:49-05:00"
    },
    {
      "path": "wrapup.html",
      "title": "Wrap-up and Conclusion",
      "author": [],
      "contents": "\nReference:\n\n\n\n",
      "last_modified": "2024-04-21T17:14:43-05:00"
    }
  ],
  "collections": []
}
