---
title: "Bayesian Inference and INLA"
description: |
editor_options: 
  markdown: 
    wrap: 72
---

# Bayesian Inference

Bayesian inference seeks to derive predictive or posterior distributions
for models, representing the probability of parameters given observed
data, or for predicting new data. The posterior distribution is computed
by multiplying the data likelihood with the prior distributions and
normalizing them to integrate into one.

In a Bayesian framework, we first specify a likelihood function
$f(y|x)$, describing the probability of observing data
$y = (y_1, \dots, y_n)$ given parameters $x$. We assign a prior
distribution $f(x|\theta)$ to $x$, where $\theta$ represents
hyperparameters governing the parameter distribution. These priors
encode our knowledge about $x$ before observing $y$. If hyperparameters
$\theta$ are unknown, a fully Bayesian approach involves specifying a
hyperprior distribution for $\theta$, or employing empirical Bayes with
estimated $\theta$. Assuming $\theta$ is known, inference about $x$
relies on Bayes' Theorem: $$
f ( x | y ) = \frac{f( y | x )  f(x)}{f(y)}
$$ The denominator represents the marginal likelihood of $y$, acting as
a scaling constant, leading to the proportional posterior distribution:
$$
f ( x| y ) \propto f( y | x )  f(x)
$$

Bayesian methods integrate prior beliefs into the model, updating them
with observed data and accommodating complex models. However, the
integral in the denominator is often intractable, so methods like MCMC
are employed to sample from conditionals and estimate the marginal
distribution for each parameter of interest.

# What is Markov Chain Monte Carlo (MCMC)?

While there are no closed-form expressions for posterior distribution,
Markov chain Monte Carlo (MCMC) methods have been traditionally used to
solve this problem. MCMC is a simulation method, addressing cases where
closed-form expressions for posterior distributions are unavailable.
This is also the method we will generally learn in class. It simulates a
Markov chain that converges to the desired posterior distribution by
generating a sequence of parameter values MCMC avoids explicit
integration of the challenging or impossible marginal likelihood
function by proposing new parameter values based on current ones, guided
by an acceptance probability ensuring convergence to the stationary
distribution.

Once convergence is achieved, sampled parameter values estimate key
characteristics of the posterior distribution. Diagnostics like trace
plots monitor mixing and convergence, ensuring reliable inference. MCMC
provides a robust framework for Bayesian inference, enabling efficient
analysis of complex statistical problems. The MCMC method has made a
great impact on statistical practice by making Bayesian inference
possible for complex models and high-dimensional data. However, since it
is a sampling simulation method that is extremely computationally
demanding, it may be slow to converge and can take a long time to
execute.

# An Alternative -- INLA Method

**Integrated nested Laplace approximation (INLA)** offers a
computationally efficient alternative to MCMC for approximate Bayesian
inference in latent Gaussian models (Håvard Rue, Martino, and Chopin
2009). While MCMC relies on the convergence of a Markov chain to
approximate the desired posterior distribution, INLA takes a different
approach. It employs a Laplacian approximation to estimate the
individual posterior marginals of model parameters. This method combines
analytical approximations with numerical algorithms for sparse matrices,
enabling the approximation of posterior distributions with closed-form
expressions. This facilitates faster inference and avoids issues related
to sample convergence in MCMC, allowing for the analysis of large
datasets and exploration of alternative models.

INLA allows to perform approximate Bayesian inference in latent Gaussian
models such as generalized linear mixed models and spatial models.
Specifically, models are of the form:

$$
y_i|x,θ∼f(y_i|x_i,θ),\ i=1,…,n,\\
x|θ \sim N(μ(θ),Q(θ)^{−1}),\\
θ∼f(θ)
$$

where,

-   $y$: the observed data, which assumed to follow the exponential
    family distribution

-   $x$: represents the latent Gaussian field, which the distribution is
    assumed to be Gaussian Markov random field (GMRF). This $x$ is
    different than the $x$ we used in the spatial model notations.

-   $\theta$: hyperparameters, which always govern the distribution of
    other parameters in the model

-   $\mu(\theta)$: the mean of the latent Gaussian field $x$

-   $Q(θ)$: the precision matrix, which is the inverse of the covariance
    matrix. Intuitively, it quantifies the relationships and
    dependencies between parameters and hyperparameters in the model.
    This allows us to model complex interactions and correlations in the
    data.

Each observation, $y_i$, linked to the linear predictors, $\eta_i$,
through a suitable link function with a mean value,
$\mu_i=g^{-1}(\eta_i)$. This linkage facilitates the connection between
observed means and the underlying linear structure, crucial for
understanding data relationships. The linear predictor, $\eta_i$,
encompasses various terms, including fixed effects and random effects,
forming the basis of the linear structure. To be more specific:

$$
η_i=\beta_0+∑^{nk}_{k=1}β_k z_{ik}+∑^{n_f}_{j=1}f^{(j)}(u_{ji})
$$

where,

-   $\beta_0$: the intercept

-   $\beta_k$: a set of coefficients of covariates $z_{ik}$,

-   $f^{(j)}$: a set of random effects. In BYM spatial model, this term
    will represent the sum of spatial and non-spatial random effects,
    which we specified in previous sections.

Moreover, given the vector of latent effects, $x$, and the
hyperparameters, $\theta$, it is assumed that the observations are
independent of each other. This implies that the **likelihood function**
can be expressed as:

$$
f(y|x,\theta)=\prod_{i \in I} f(y_i|\eta_i, \theta)
$$

where,

-   $I$: contains the indices for all observed values of $y$. In other
    words, the indices of the observations for which we have actual
    data.

INLA uses a combination of analytically approximations and numerical
integration to obtain approximated posterior distributions of the
parameters. INLA methodology approximate the posterior marginals of
model parameters and hyperparameters, which estimates the distribution
of each parameter and hyperparameter separately within the overall
posterior distribution. This allows us to understand the uncertainty
associated with each parameter and hyperparameter independently. This is
achieved by exploiting the computational properties of GMRF and the
Laplace approximation for multidimensional integration.

The joint posterior distribution of the effects and hyperparameters can
be expressed as:

$$
f(x,\theta|y)\propto f(θ)f(x|\theta) \prod_{i \in I} f(y_i|x_i, \theta)\\
\propto f(\theta)|Q(\theta)|^{\frac{1}{2}} exp \{ −\frac{1}{2}x^\top Q(\theta)x + \sum_{i \in I}log(f(y_i|x_i,\theta)) \}.
$$

where,

-   $x_i = \eta_i$ when $i \in I$, since mentioned above $\eta_i$ is
    part of vector $x$. This implies that for observations in the set of
    observed values, the latent effects $x_i$​ are directly equal to the
    corresponding latent linear predictors $\eta_i$

Now we can calculate the **marginal distributions** (MAIN GOAL!!) for
the latent Gaussian variables, $x$, and hyperparameter, $\theta$. It can
be done considering that

$$
f(x_i|y)= \int f(x_i|\theta,y)f(\theta|y)d\theta, \ i = 1, \dots,n \\
f(\theta_j | y ) = \int f ( \theta | y ) d\theta_ {−j},\ j=1,…,dim(θ).
$$

​This provides us a way to obtain the marginal distribution of the latent
effect $x_i$ (model parameters) and a specific hyperparameter $\theta_j$​
from the joint posterior distribution of all hyperparameters given the
observed data $y$.

This nested formulation, which reflects the layered approach of
approximating the posterior distributions of latent effects and
hyperparameters, is used to approximate $f(x_i|y)$ by combining
analytical approximations to the full conditionals $f(x_i|\theta,y)$ and
$f(\theta|y)$ and numerical integration routines to integrate out
$\theta$. Similarly, $f(\theta_j|y)$ is approximated by approximating
$f(\theta|y)$ and integrating out $\theta_{-j}$. To construct nested
approximations by INLA, it can be written as

$$
\tilde f(x_i|y)= \int \tilde f(x_i|\theta,y) \tilde f(\theta|y)d\theta\\
\tilde f(\theta_j | y ) = \int \tilde f ( \theta | y ) d\theta_ {−j}
$$

where $\tilde f( \cdot| \cdot)$ is an approximated posterior density.
The approximation to the marginal density $f(x_i|y)$ is obtained in a
nested fashion by first approximating $f(\theta|y)$ and
$f(x_i|\theta,y)$, and then numerically integrating out $\theta$ as

$$
f(x_i|y)=\sum_k \tilde f(x_i|\theta_k,y)\times\tilde f(\theta_k|y)\times \Delta_k,
$$

where

-   $\Delta_k$: represents the integration weights associated with each
    value of $\theta_k$​, which also denotes the area weight
    corresponding to $\theta_k$.

-   $\sum _k$: the summation of all possible values of $\theta$ with
    each term weighted by the respective integration weight
    $\Delta_{k}$.

The posterior marginals for the $x_i$ conditioned on selected values of
$\theta_k$​, denoted by $f(x_i|\theta_k,y)$, and the approximation
$\tilde f (\theta_k|y)$ can be obtained using various approximation
techniques such as Gaussian, Laplace, or simplified Laplace
approximations. The Gaussian approximation, derived from
$\tilde f_G(x|\theta,y)$, is the simplest and fastest solution.

However, it may introduce errors in location and fail to capture
skewness behavior in certain situations. The Laplace approximation,
while preferable to the Gaussian one, is relatively more computationally
expensive. Alternatively, the simplified Laplace approximation provides
a middle-ground between computational cost and accuracy, which is also
the default option in the R-INLA package.
